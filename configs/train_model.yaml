# Data params
data_dir: product_clustering_fluent2_bucket/tree/cactus/amazon_dataset
category: Electronics
is_manifold: true
local_dir_path: ../amazon_review_local_dir # relative to out dir
manifold_out_dir: product_clustering_fluent2_bucket/tree/cactus/outputs/train_model_with_cf
cf_vector_base_dir: product_clustering_fluent2_bucket/tree/cactus/outputs/train_autoencoder/train_autoencoder_Electronics_20210913_005225

# Dataset params
train_set_ratio: 0.75
seed: 1234
num_workers: 8
dpp_server_num_worker_threads: 8
img_transform_resize: 256 # Aligned with imagenet
img_transform_crop: 224 # Aligned with imagenet
is_use_cf_bias: true
labeled_ratio: 1.0 # The ratio fo the traninig set to contain label

# Optimization params
batch_size: 256
weight_decay: 1e-4
cf_weight: 0.0 # The loss weight for cf vectors
is_pretrained: true
unfreeze_backbone_at_epoch: 1.0 # Epoch at which the backbone will be unfreezed.
backbone_initial_lr: 1e-1 # Inital learning rate for the backbone.
backbone_lr_multiplicative: 10 # Scheduling function for increasing backbone learning rate.
lr: 1e-1
train_set_repeat: 5 # each is trainning 1 + train_set_repeat times with the dataloder
milestones: [10, 14] # effective milestones: train_set_repeat * milestones
epochs: 15 # effective epochs: train_set_repeat * epochs
print_interval: 1
cf_topk_loss_ratio: 0.9 # Consider only items with the lowest loss in the batch. 1.0 means consider all

# Flow params
is_debug: false
gpu: 0

hydra:
  run:
    dir: /home/kbibas/cactus_outputs/train_model_${category}_${now:%Y%m%d_%H%M%S}
  sweep:
    dir: /home/kbibas/cactus_outputs/train_model_multirun_${category}_${now:%Y%m%d_%H%M%S}
    subdir: train_model_${category}_${now:%Y%m%d_%H%M%S}_${hydra.job.num}

# Hyper-params:
# buck run @mode/dev-nosan :train_model -- -m gpu=0 labeled_ratio=1.0,0.8,0.6,0.4,0.2
# buck run @mode/dev-nosan :train_model -- -m gpu=1 labeled_ratio=0.2 cf_weight=0.1,1,2,10,20,100,200,1000

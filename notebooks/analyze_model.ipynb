{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import os.path as osp\n",
    "import yaml\n",
    "import sys\n",
    "import logging\n",
    "from glob import glob\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.metrics import average_precision_score\n",
    "sys.path.append(\"../src\")\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset_utils import get_datasets\n",
    "from lit_utils import LitModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [\n",
    "    \"../outputs/train_model_Clothing_Shoes_and_Jewelry_20211027_102744\",\n",
    "    \"../outputs/train_model_multirun_Clothing_Shoes_and_Jewelry_20211020_172806/train_model_Clothing_Shoes_and_Jewelry_20211020_172806_0\",\n",
    "    # \"../outputs/train_model_multirun_Clothing_Shoes_and_Jewelry_20211020_172806/train_model_Clothing_Shoes_and_Jewelry_20211020_172806_1\",\n",
    "    \"../outputs/train_model_multirun_Clothing_Shoes_and_Jewelry_20211020_172806/train_model_Clothing_Shoes_and_Jewelry_20211020_172806_2\",\n",
    "    # \"../outputs/train_model_multirun_Clothing_Shoes_and_Jewelry_20211020_172806/train_model_Clothing_Shoes_and_Jewelry_20211020_172806_3\",\n",
    "]\n",
    "\n",
    "# Cosine loss\n",
    "paths = [\n",
    "'../outputs/train_model_multirun_Clothing_Shoes_and_Jewelry_20211027_141006/train_model_Clothing_Shoes_and_Jewelry_20211027_141006_0',\n",
    "'../outputs/train_model_multirun_Clothing_Shoes_and_Jewelry_20211027_141006/train_model_Clothing_Shoes_and_Jewelry_20211027_141006_1',\n",
    "'../outputs/train_model_multirun_Clothing_Shoes_and_Jewelry_20211027_141006/train_model_Clothing_Shoes_and_Jewelry_20211027_141006_2',\n",
    "'../outputs/train_model_multirun_Clothing_Shoes_and_Jewelry_20211027_141006/train_model_Clothing_Shoes_and_Jewelry_20211027_141006_3',\n",
    "]\n",
    "\n",
    "# Inner product\n",
    "paths = [\n",
    "'../outputs/train_model_multirun_Clothing_Shoes_and_Jewelry_20211028_090148/train_model_Clothing_Shoes_and_Jewelry_20211028_090148_0',\n",
    "'../outputs/train_model_multirun_Clothing_Shoes_and_Jewelry_20211028_090148/train_model_Clothing_Shoes_and_Jewelry_20211028_090148_1',\n",
    "'../outputs/train_model_multirun_Clothing_Shoes_and_Jewelry_20211028_090148/train_model_Clothing_Shoes_and_Jewelry_20211028_090148_2'\n",
    "]\n",
    "\n",
    "# Less labeled data\n",
    "paths= [\n",
    "'../outputs/train_model_multirun_Clothing_Shoes_and_Jewelry_20211029_151409/train_model_Clothing_Shoes_and_Jewelry_20211029_151409_0',\n",
    "'../outputs/train_model_multirun_Clothing_Shoes_and_Jewelry_20211029_151409/train_model_Clothing_Shoes_and_Jewelry_20211029_151409_1',\n",
    "'../outputs/train_model_multirun_Clothing_Shoes_and_Jewelry_20211029_151409/train_model_Clothing_Shoes_and_Jewelry_20211029_151409_2',\n",
    "'../outputs/train_model_multirun_Clothing_Shoes_and_Jewelry_20211029_151409/train_model_Clothing_Shoes_and_Jewelry_20211029_151409_3',\n",
    "'../outputs/train_model_multirun_Clothing_Shoes_and_Jewelry_20211029_151409/train_model_Clothing_Shoes_and_Jewelry_20211029_151409_4',\n",
    "'../outputs/train_model_multirun_Clothing_Shoes_and_Jewelry_20211030_080601/train_model_Clothing_Shoes_and_Jewelry_20211030_080601_0',\n",
    "'../outputs/train_model_multirun_Clothing_Shoes_and_Jewelry_20211030_080601/train_model_Clothing_Shoes_and_Jewelry_20211030_080601_1',\n",
    "'../outputs/train_model_multirun_Clothing_Shoes_and_Jewelry_20211030_080601/train_model_Clothing_Shoes_and_Jewelry_20211030_080601_2',\n",
    "'../outputs/train_model_multirun_Clothing_Shoes_and_Jewelry_20211030_080601/train_model_Clothing_Shoes_and_Jewelry_20211030_080601_3',\n",
    "'../outputs/train_model_multirun_Clothing_Shoes_and_Jewelry_20211030_080601/train_model_Clothing_Shoes_and_Jewelry_20211030_080601_4',\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../outputs/train_model_multirun_Clothing_Shoes_and_Jewelry_20211029_151409/train_model_Clothing_Shoes_and_Jewelry_20211029_151409_0\n",
      "cfg[\"cf_weight\"]=0.0 cfg[\"labeled_ratio\"]=0.5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "784be7fe3cbf46348acd18917d1bdc53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no_labels=121\n",
      "recall=0.691 ap=0.341\n",
      "../outputs/train_model_multirun_Clothing_Shoes_and_Jewelry_20211029_151409/train_model_Clothing_Shoes_and_Jewelry_20211029_151409_1\n",
      "cfg[\"cf_weight\"]=0.0 cfg[\"labeled_ratio\"]=0.6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aab85ec4157147dea3841e128f34d65f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no_labels=121\n",
      "recall=0.689 ap=0.348\n",
      "../outputs/train_model_multirun_Clothing_Shoes_and_Jewelry_20211029_151409/train_model_Clothing_Shoes_and_Jewelry_20211029_151409_2\n",
      "cfg[\"cf_weight\"]=0.0 cfg[\"labeled_ratio\"]=0.7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5af7d9a213f84f129d81dd9503f7c4c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no_labels=121\n",
      "recall=0.687 ap=0.352\n",
      "../outputs/train_model_multirun_Clothing_Shoes_and_Jewelry_20211029_151409/train_model_Clothing_Shoes_and_Jewelry_20211029_151409_3\n",
      "cfg[\"cf_weight\"]=0.0 cfg[\"labeled_ratio\"]=0.8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5fdd5624b0e4432abfecd5169a4ad0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no_labels=121\n",
      "recall=0.686 ap=0.359\n",
      "../outputs/train_model_multirun_Clothing_Shoes_and_Jewelry_20211029_151409/train_model_Clothing_Shoes_and_Jewelry_20211029_151409_4\n",
      "cfg[\"cf_weight\"]=0.0 cfg[\"labeled_ratio\"]=0.9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0919e864f4234c16b75b7f176f6dc8d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no_labels=121\n",
      "recall=0.229 ap=0.13\n",
      "../outputs/train_model_multirun_Clothing_Shoes_and_Jewelry_20211030_080601/train_model_Clothing_Shoes_and_Jewelry_20211030_080601_0\n",
      "cfg[\"cf_weight\"]=2.0 cfg[\"labeled_ratio\"]=0.5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "544b9684964c4a619c3583d77c34eaee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no_labels=121\n",
      "recall=0.693 ap=0.347\n",
      "../outputs/train_model_multirun_Clothing_Shoes_and_Jewelry_20211030_080601/train_model_Clothing_Shoes_and_Jewelry_20211030_080601_1\n",
      "cfg[\"cf_weight\"]=2.0 cfg[\"labeled_ratio\"]=0.6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb4f2145126e48e3bd37cbc933307080",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no_labels=121\n",
      "recall=0.693 ap=0.356\n",
      "../outputs/train_model_multirun_Clothing_Shoes_and_Jewelry_20211030_080601/train_model_Clothing_Shoes_and_Jewelry_20211030_080601_2\n",
      "cfg[\"cf_weight\"]=2.0 cfg[\"labeled_ratio\"]=0.7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "355dd2b319ac4c61907587a767f0e599",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no_labels=121\n",
      "recall=0.692 ap=0.36\n",
      "../outputs/train_model_multirun_Clothing_Shoes_and_Jewelry_20211030_080601/train_model_Clothing_Shoes_and_Jewelry_20211030_080601_3\n",
      "cfg[\"cf_weight\"]=2.0 cfg[\"labeled_ratio\"]=0.8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dbd2461ee32438f9a8512e65187113f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no_labels=121\n",
      "recall=0.692 ap=0.36\n",
      "../outputs/train_model_multirun_Clothing_Shoes_and_Jewelry_20211030_080601/train_model_Clothing_Shoes_and_Jewelry_20211030_080601_4\n",
      "cfg[\"cf_weight\"]=2.0 cfg[\"labeled_ratio\"]=0.9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acb56fb3c6e048e3b6d2fbe6a3979ebb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no_labels=121\n",
      "recall=0.691 ap=0.367\n"
     ]
    }
   ],
   "source": [
    "res_dict = {}\n",
    "recall_dict, hit_dict = {}, {}\n",
    "for path in paths:\n",
    "    print(path)\n",
    "    config_path = osp.join(path, \".hydra\", \"config.yaml\")\n",
    "    model_path = glob(osp.join(path, \"epoch=*.ckpt*\"))[0]\n",
    "\n",
    "    # Load config file\n",
    "    with open(config_path, \"r\") as f:\n",
    "        cfg = yaml.safe_load(f)\n",
    "    cfg[\"is_pretrained\"] = False\n",
    "    cfg[\"batch_size\"] = 128\n",
    "    print(f'{cfg[\"cf_weight\"]=} {cfg[\"labeled_ratio\"]=}')\n",
    "\n",
    "    # Load dataset\n",
    "    train_dataset, test_dataset, dataset_meta, pos_weight = get_datasets(\n",
    "        cfg[\"train_df_path\"],\n",
    "        cfg[\"test_df_path\"],\n",
    "        cfg[\"cf_vector_df_path\"],\n",
    "        cfg[\"labeled_ratio\"],\n",
    "    )\n",
    "    logger.info(\n",
    "        \"Sizes [trainset testset num_classes]=[{} {} {}]\".format(\n",
    "            dataset_meta[\"train_set_size\"],\n",
    "            dataset_meta[\"test_set_size\"],\n",
    "            dataset_meta[\"num_classes\"],\n",
    "        )\n",
    "    )\n",
    "\n",
    "    testloader = DataLoader(\n",
    "        test_dataset, batch_size=cfg[\"batch_size\"], num_workers=cfg[\"num_workers\"]\n",
    "    )\n",
    "\n",
    "    # Load model\n",
    "    lit_h = LitModel.load_from_checkpoint(model_path)\n",
    "    device = \"cuda:0\"\n",
    "    lit_h = lit_h.to(device)\n",
    "    lit_h = lit_h.eval()\n",
    "\n",
    "    # Get predictions\n",
    "    label_list, pred_list = [], []\n",
    "    for batch in tqdm(testloader):\n",
    "\n",
    "        (\n",
    "            imgs,\n",
    "            _,\n",
    "            labels,\n",
    "            _,\n",
    "        ) = batch\n",
    "\n",
    "        imgs = imgs.to(device)\n",
    "        y_hat, _ = lit_h(imgs)\n",
    "        preds = torch.sigmoid(y_hat)\n",
    "\n",
    "        pred_list.append(preds.detach().cpu())\n",
    "        label_list.append(labels.detach().cpu())\n",
    "    preds = torch.vstack(pred_list).numpy()\n",
    "    labels = torch.vstack(label_list).numpy()\n",
    "\n",
    "    # Score\n",
    "    ap = average_precision_score(labels, preds, average=None)\n",
    "\n",
    "    # Recall\n",
    "    recall, hit = 0, 0\n",
    "    items = 0\n",
    "    no_labels = 0\n",
    "    for pred, label in zip(torch.tensor(preds), torch.tensor(labels)):\n",
    "        _, pred_idx = torch.topk(pred, k=5)\n",
    "        label_idx = torch.where(label == 1)[0]\n",
    "\n",
    "        if len(label_idx) == 0:\n",
    "            no_labels += 1\n",
    "            continue\n",
    "\n",
    "        recall_i = sum(el in pred_idx for el in label_idx) / len(label_idx)\n",
    "        recall += recall_i\n",
    "\n",
    "        hit_i = sum(el in label_idx for el in pred_idx)\n",
    "        hit += hit_i\n",
    "\n",
    "        items += 1\n",
    "\n",
    "    recall /= items\n",
    "    hit /= items\n",
    "\n",
    "    recall_dict[f'cf_weight={cfg[\"cf_weight\"]} labeled_ratio={cfg[\"labeled_ratio\"]}'] = recall\n",
    "    hit_dict[f'cf_weight={cfg[\"cf_weight\"]} labeled_ratio={cfg[\"labeled_ratio\"]}'] = hit\n",
    "\n",
    "    print(f\"{no_labels=}\")\n",
    "    print(f'recall={np.round(recall,3)} ap={np.round(ap.mean(),3)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cf_weight=0.0 labeled_ratio=0.5': 0.6911625390745033,\n",
       " 'cf_weight=0.0 labeled_ratio=0.6': 0.6885688226997471,\n",
       " 'cf_weight=0.0 labeled_ratio=0.7': 0.6869218466509665,\n",
       " 'cf_weight=0.0 labeled_ratio=0.8': 0.6858133034092401,\n",
       " 'cf_weight=0.0 labeled_ratio=0.9': 0.22890082703852438,\n",
       " 'cf_weight=2.0 labeled_ratio=0.5': 0.692607163758405,\n",
       " 'cf_weight=2.0 labeled_ratio=0.6': 0.6933390608221298,\n",
       " 'cf_weight=2.0 labeled_ratio=0.7': 0.692049154340351,\n",
       " 'cf_weight=2.0 labeled_ratio=0.8': 0.6921553327535273,\n",
       " 'cf_weight=2.0 labeled_ratio=0.9': 0.6909699380128276}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../outputs/train_model_multirun_Clothing_Shoes_and_Jewelry_20211029_151409/train_model_Clothing_Shoes_and_Jewelry_20211029_151409_0\n",
      "cfg[\"cf_weight\"]=0.0 cfg[\"labeled_ratio\"]=0.5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03a6535c36f142d2a71cb84d1a11586a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no_labels=121\n",
      "recall=0.691 ap=0.341\n"
     ]
    }
   ],
   "source": [
    "res_dict = {}\n",
    "recall_dict, hit_dict = {}, {}\n",
    "for path in paths:\n",
    "    print(path)\n",
    "    config_path = osp.join(path, \".hydra\", \"config.yaml\")\n",
    "    model_path = glob(osp.join(path, \"epoch=*.ckpt*\"))[0]\n",
    "\n",
    "    # Load config file\n",
    "    with open(config_path, \"r\") as f:\n",
    "        cfg = yaml.safe_load(f)\n",
    "    cfg[\"is_pretrained\"] = False\n",
    "    cfg[\"batch_size\"] = 128\n",
    "    print(f'{cfg[\"cf_weight\"]=} {cfg[\"labeled_ratio\"]=}')\n",
    "\n",
    "    # Load dataset\n",
    "    train_dataset, test_dataset, dataset_meta, pos_weight = get_datasets(\n",
    "        cfg[\"train_df_path\"],\n",
    "        cfg[\"test_df_path\"],\n",
    "        cfg[\"cf_vector_df_path\"],\n",
    "        cfg[\"labeled_ratio\"],\n",
    "    )\n",
    "    logger.info(\n",
    "        \"Sizes [trainset testset num_classes]=[{} {} {}]\".format(\n",
    "            dataset_meta[\"train_set_size\"],\n",
    "            dataset_meta[\"test_set_size\"],\n",
    "            dataset_meta[\"num_classes\"],\n",
    "        )\n",
    "    )\n",
    "\n",
    "    testloader = DataLoader(\n",
    "        test_dataset, batch_size=cfg[\"batch_size\"], num_workers=cfg[\"num_workers\"]\n",
    "    )\n",
    "\n",
    "    # Load model\n",
    "    lit_h = LitModel.load_from_checkpoint(model_path)\n",
    "    device = \"cuda:0\"\n",
    "    lit_h = lit_h.to(device)\n",
    "    lit_h = lit_h.eval()\n",
    "\n",
    "    # Get predictions\n",
    "    label_list, pred_list = [], []\n",
    "    for batch in tqdm(testloader):\n",
    "\n",
    "        (\n",
    "            imgs,\n",
    "            _,\n",
    "            labels,\n",
    "            _,\n",
    "        ) = batch\n",
    "\n",
    "        imgs = imgs.to(device)\n",
    "        y_hat, _ = lit_h(imgs)\n",
    "        preds = torch.sigmoid(y_hat)\n",
    "\n",
    "        pred_list.append(preds.detach().cpu())\n",
    "        label_list.append(labels.detach().cpu())\n",
    "    preds = torch.vstack(pred_list).numpy()\n",
    "    labels = torch.vstack(label_list).numpy()\n",
    "\n",
    "    # Score\n",
    "    ap = average_precision_score(labels, preds, average=None)\n",
    "\n",
    "    # Recall\n",
    "    recall, hit = 0, 0\n",
    "    items = 0\n",
    "    no_labels = 0\n",
    "    for pred, label in zip(torch.tensor(preds), torch.tensor(labels)):\n",
    "        _, pred_idx = torch.topk(pred, k=5)\n",
    "        label_idx = torch.where(label == 1)[0]\n",
    "\n",
    "        if len(label_idx) == 0:\n",
    "            no_labels += 1\n",
    "            continue\n",
    "\n",
    "        recall_i = sum(el in pred_idx for el in label_idx) / len(label_idx)\n",
    "        recall += recall_i\n",
    "\n",
    "        hit_i = sum(el in label_idx for el in pred_idx)\n",
    "        hit += hit_i\n",
    "\n",
    "        items += 1\n",
    "\n",
    "    recall /= items\n",
    "    hit /= items\n",
    "\n",
    "    recall_dict[f'cf_weight={cfg[\"cf_weight\"]} labeled_ratio={cfg[\"labeled_ratio\"]}'] = recall\n",
    "    hit_dict[f'cf_weight={cfg[\"cf_weight\"]} labeled_ratio={cfg[\"labeled_ratio\"]}'] = hit\n",
    "\n",
    "    print(f\"{no_labels=}\")\n",
    "    print(f'recall={np.round(recall,3)} ap={np.round(ap.mean(),3)}')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(el in pred_idx for el in label_idx) / len(label_idx)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 28,  32,  25, 195,  22]), tensor([22, 25, 28, 32]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_idx, label_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', res_df.shape[0]+1)\n",
    "\n",
    "df = res_df[['cf_weight=0.0', 'cf_weight=2.0', 'count']]\n",
    "df['diff'] =  df['cf_weight=2.0'] - df['cf_weight=0.0']\n",
    "df = df.sort_values(by='diff')\n",
    "df= df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,1,figsize=(30,10))\n",
    "ax = df[['cf_weight=0.0', 'cf_weight=2.0']].plot.bar(rot=0,ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df['count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topk= 5\n",
    "\n",
    "idx_top = preds.argsort()[::-1][:topk]\n",
    "label_top = labels.argsort()[::-1][:topk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, idx_top =  torch.topk(torch.tensor(preds),k=topk,dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n,idxs in enumerate(idx_top):\n",
    "    label_in_idx_top = labels[n][idxs]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.functional import recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall(torch.tensor(preds),torch.tensor(labels),top_k=5,average=None,num_classes=preds.shape[-1]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [[0.99,0.99,0.99,0.99,0.99,0,0,0]]\n",
    "b = [[1,1,1,1,1,0,0,0]]\n",
    "recall(torch.tensor(a),torch.tensor(b),top_k=5,average=None,num_classes=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,idx = torch.topk(torch.tensor(a),k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_idx,label_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "93276fba55acec72ebf09ca3f12064a0f60315826507a886c17c3c32ba5acdb2"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('cactus': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

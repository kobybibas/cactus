{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "from omegaconf import OmegaConf\n",
    "import numpy as np\n",
    "from sklearn.metrics import average_precision_score, f1_score\n",
    "from scipy.stats import ttest_rel\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "eps = np.finfo(\"float\").eps\n",
    "\n",
    "plt.style.use([\"science\", \"ieee\"])\n",
    "out_path = osp.join(\"../outputs/figures\")\n",
    "metric_res_dicts_path = osp.join(out_path, \"metric_res_dicts.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_label_ratio_dicts(results_path):\n",
    "    res_dict = OmegaConf.load(results_path)\n",
    "\n",
    "    # Build absolute path\n",
    "    res_dict = {\n",
    "        key: osp.join(res_dict[\"base_path\"], value)\n",
    "        for key, value in res_dict.items()\n",
    "        if key != \"base_path\"\n",
    "    }\n",
    "\n",
    "    no_cf_dict = {key: value for key, value in res_dict.items() if \"_no_cf\" in key}\n",
    "    with_cf_dict = {key: value for key, value in res_dict.items() if \"_with_cf\" in key}\n",
    "    return no_cf_dict, with_cf_dict\n",
    "\n",
    "\n",
    "def load_preds(base_path):\n",
    "    no_cf_dict, with_cf_dict = build_label_ratio_dicts(base_path)\n",
    "\n",
    "    labels = np.load(osp.join(list(no_cf_dict.values())[0], \"labels.npy\"))\n",
    "\n",
    "    no_cf_preds, with_cf_preds = [], []\n",
    "    for (key_a, path_a), (key_b, path_b) in zip(\n",
    "        no_cf_dict.items(), with_cf_dict.items()\n",
    "    ):\n",
    "        preds_a = np.load(osp.join(path_a, \"preds.npy\"))\n",
    "        preds_b = np.load(osp.join(path_b, \"preds.npy\"))\n",
    "\n",
    "        ap_a = average_precision_score(labels, preds_a)  # ,average='micro')\n",
    "        ap_b = average_precision_score(labels, preds_b)  # ,average='micro')\n",
    "\n",
    "        print(f\"{key_a} {key_b} [{ap_a:.3f} {ap_b:.3f}]. size={len(preds_a)}\")\n",
    "        no_cf_preds.append(preds_a)\n",
    "        with_cf_preds.append(preds_b)\n",
    "\n",
    "    return {\n",
    "        \"no_cf_preds\": no_cf_preds,\n",
    "        \"with_cf_preds\": with_cf_preds,\n",
    "        \"labels\": labels,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beauty\n",
      "label_ratio_0.1_no_cf label_ratio_0.1_with_cf [0.070 0.052]. size=3624\n",
      "label_ratio_0.2_no_cf label_ratio_0.2_with_cf [0.239 0.238]. size=3624\n",
      "label_ratio_0.3_no_cf label_ratio_0.3_with_cf [0.268 0.271]. size=3624\n",
      "label_ratio_0.4_no_cf label_ratio_0.4_with_cf [0.301 0.296]. size=3624\n",
      "label_ratio_0.5_no_cf label_ratio_0.5_with_cf [0.314 0.319]. size=3624\n",
      "label_ratio_0.6_no_cf label_ratio_0.6_with_cf [0.329 0.330]. size=3624\n",
      "label_ratio_0.7_no_cf label_ratio_0.7_with_cf [0.327 0.330]. size=3624\n",
      "label_ratio_0.8_no_cf label_ratio_0.8_with_cf [0.344 0.338]. size=3624\n",
      "label_ratio_0.9_no_cf label_ratio_0.9_with_cf [0.349 0.353]. size=3624\n",
      "label_ratio_1.0_no_cf label_ratio_1.0_with_cf [0.354 0.358]. size=3624\n",
      "Toys_and_Games\n",
      "label_ratio_0.1_no_cf label_ratio_0.1_with_cf [0.078 0.098]. size=3572\n",
      "label_ratio_0.2_no_cf label_ratio_0.2_with_cf [0.168 0.183]. size=3572\n",
      "label_ratio_0.3_no_cf label_ratio_0.3_with_cf [0.295 0.294]. size=3572\n",
      "label_ratio_0.4_no_cf label_ratio_0.4_with_cf [0.315 0.324]. size=3572\n",
      "label_ratio_0.5_no_cf label_ratio_0.5_with_cf [0.343 0.359]. size=3572\n",
      "label_ratio_0.6_no_cf label_ratio_0.6_with_cf [0.358 0.377]. size=3572\n",
      "label_ratio_0.7_no_cf label_ratio_0.7_with_cf [0.370 0.393]. size=3572\n",
      "label_ratio_0.8_no_cf label_ratio_0.8_with_cf [0.390 0.398]. size=3572\n",
      "label_ratio_0.9_no_cf label_ratio_0.9_with_cf [0.404 0.414]. size=3572\n",
      "label_ratio_1.0_no_cf label_ratio_1.0_with_cf [0.400 0.411]. size=3572\n",
      "Clothing_Shoes_and_Jewelry\n",
      "label_ratio_0.1_no_cf label_ratio_0.1_with_cf [0.195 0.225]. size=6766\n",
      "label_ratio_0.2_no_cf label_ratio_0.2_with_cf [0.274 0.294]. size=6766\n",
      "label_ratio_0.3_no_cf label_ratio_0.3_with_cf [0.297 0.313]. size=6766\n",
      "label_ratio_0.4_no_cf label_ratio_0.4_with_cf [0.322 0.331]. size=6766\n",
      "label_ratio_0.5_no_cf label_ratio_0.5_with_cf [0.337 0.342]. size=6766\n",
      "label_ratio_0.6_no_cf label_ratio_0.6_with_cf [0.345 0.357]. size=6766\n",
      "label_ratio_0.7_no_cf label_ratio_0.7_with_cf [0.357 0.364]. size=6766\n",
      "label_ratio_0.8_no_cf label_ratio_0.8_with_cf [0.358 0.369]. size=6766\n",
      "label_ratio_0.9_no_cf label_ratio_0.9_with_cf [0.367 0.371]. size=6766\n",
      "label_ratio_1.0_no_cf label_ratio_1.0_with_cf [0.371 0.375]. size=6766\n",
      "movielens\n",
      "label_ratio_0.1_no_cf label_ratio_0.1_with_cf [0.263 0.269]. size=3183\n",
      "label_ratio_0.2_no_cf label_ratio_0.2_with_cf [0.274 0.281]. size=3183\n",
      "label_ratio_0.3_no_cf label_ratio_0.3_with_cf [0.284 0.292]. size=3183\n",
      "label_ratio_0.4_no_cf label_ratio_0.4_with_cf [0.298 0.301]. size=3183\n",
      "label_ratio_0.5_no_cf label_ratio_0.5_with_cf [0.310 0.313]. size=3183\n",
      "label_ratio_0.6_no_cf label_ratio_0.6_with_cf [0.310 0.317]. size=3183\n",
      "label_ratio_0.7_no_cf label_ratio_0.7_with_cf [0.314 0.323]. size=3183\n",
      "label_ratio_0.8_no_cf label_ratio_0.8_with_cf [0.319 0.322]. size=3183\n",
      "label_ratio_0.9_no_cf label_ratio_0.9_with_cf [0.322 0.328]. size=3183\n",
      "label_ratio_1.0_no_cf label_ratio_1.0_with_cf [0.326 0.331]. size=3183\n"
     ]
    }
   ],
   "source": [
    "dataset_mapping = {\n",
    "    \"Beauty\": \"Beauty\",\n",
    "    \"Toys_and_Games\": \"Toys\",\n",
    "    \"Clothing_Shoes_and_Jewelry\": \"Clothing\",\n",
    "    \"movielens\": \"MovieLens\",\n",
    "}\n",
    "\n",
    "preds_dict = {}\n",
    "for dataset_name, print_name in dataset_mapping.items():\n",
    "    print(dataset_name)\n",
    "    preds_dict[print_name] = load_preds(\n",
    "        osp.join(f\"../outputs/{dataset_name}/results.yaml\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_confidence_interval(data, confidence=0.9):\n",
    "    a = 1.0 * np.array(data)\n",
    "    n = len(a)\n",
    "    se = scipy.stats.sem(a)\n",
    "    h = se * scipy.stats.t.ppf((1 + confidence) / 2.0, n - 1)\n",
    "    return h\n",
    "\n",
    "\n",
    "def compare_results(preds_dict: dict, eval_function,metric_dict:dict):\n",
    "    for dataset_name, dataset_dict in preds_dict.items():\n",
    "        print(dataset_name)\n",
    "        if dataset_name in metric_dict:\n",
    "            print(f'{dataset_name} exists in metric_dict')\n",
    "            continue\n",
    "\n",
    "        labels = dataset_dict[\"labels\"]\n",
    "        no_cf_preds = dataset_dict[\"no_cf_preds\"]\n",
    "        with_cf_preds = dataset_dict[\"with_cf_preds\"]\n",
    "        df = single_set_compare_results(\n",
    "            labels, no_cf_preds, with_cf_preds, eval_function\n",
    "        )\n",
    "        metric_dict[dataset_name] = df\n",
    "        print(df[[\"no_cf\", \"with_cf\"]].round(2).T)\n",
    "    return metric_dict\n",
    "\n",
    "def single_label_ratio_compare_results(label_ratio,labels,preds_a, preds_b,eval_function):\n",
    "        # Define output\n",
    "        res_dict = {'label_ratio': label_ratio}\n",
    "\n",
    "        # Evaluate performance\n",
    "        perf_a = eval_function(labels, preds_a)\n",
    "        perf_b = eval_function(labels, preds_b)\n",
    "\n",
    "        res_dict[\"pvalue\"] = ttest_rel(perf_a, perf_b).pvalue\n",
    "\n",
    "        # No CF\n",
    "        res_dict[\"no_cf\"] = np.mean(perf_a)\n",
    "        res_dict[\"no_cf_std\"] = np.std(perf_a)\n",
    "        res_dict[\"no_cf_ci\"] = mean_confidence_interval(perf_a)\n",
    "\n",
    "        # With CF\n",
    "        res_dict[\"with_cf\"] = np.mean(perf_b)\n",
    "        res_dict[\"with_cf_std\"] = np.std(perf_b)\n",
    "        res_dict[\"with_cf_ci\"] = mean_confidence_interval(perf_b)\n",
    "\n",
    "        return res_dict\n",
    "\n",
    "def single_set_compare_results(\n",
    "    labels, no_cf_pred_list, with_cf_pred_list, eval_function\n",
    "):  \n",
    "\n",
    "    # Defining a dict\n",
    "    res_dicts = []\n",
    "    total = len(no_cf_pred_list)\n",
    "    label_ratios = np.arange(0.1, 1.1, 0.1)\n",
    "    for label_ratio, preds_a, preds_b in tqdm(zip(label_ratios, no_cf_pred_list, with_cf_pred_list), total=total):\n",
    "\n",
    "        res_dict = single_label_ratio_compare_results(label_ratio,labels,preds_a, preds_b,eval_function)\n",
    "        res_dicts.append(res_dict)\n",
    "\n",
    "    df = pd.DataFrame(res_dicts)\n",
    "    df.set_index('label_ratio')\n",
    "    df[\"improvement\"] = df[\"with_cf\"] / df[\"no_cf\"] - 1.0\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def plot_performance(df):\n",
    "    fig, ax = plt.subplots(1, 1, dpi=150)\n",
    "    ax.plot(df.index, df[\"no_cf\"], label=\"Baseline\")\n",
    "    ax.plot(df.index, df[\"with_cf\"], label=\"With CF\")\n",
    "\n",
    "    for i, key in enumerate([\"no_cf\", \"with_cf\"]):\n",
    "        ax.fill_between(\n",
    "            df.index,\n",
    "            df[key] - df[key + \"_ci\"],\n",
    "            df[key] + df[key + \"_ci\"],\n",
    "            color=f\"C{i}\",\n",
    "            alpha=0.1,\n",
    "        )\n",
    "\n",
    "    ax.set_xlabel(\"Label ratio\")\n",
    "    ax.legend()\n",
    "    return ax\n",
    "\n",
    "\n",
    "def plot_pvalues(df):\n",
    "    fig, ax = plt.subplots(1, 1, dpi=150)\n",
    "    ax.plot(df.index, df[\"pvalue\"])\n",
    "    plt.axhline(y=0.05, color=\"r\", linestyle=\"-\", label=\"p=0.05\")\n",
    "    ax.set_xlabel(\"Label ratio\")\n",
    "    ax.set_ylabel(\"p-value\")\n",
    "    ax.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_top1_acc(labels,preds):\n",
    "    return np.array([labels[n][top1] for n, top1 in enumerate(np.argmax(preds,axis=1))])\n",
    "\n",
    "def calc_ap_score(labels,preds):\n",
    "    aps = []\n",
    "    num_experiments = 100\n",
    "    num_samples = int(0.9 * len(labels))\n",
    "\n",
    "    idxs_list = np.random.randint(low=0,high=len(labels), size=(num_experiments,num_samples))\n",
    "    for idxs in idxs_list:\n",
    "        labels_chosen, preds_chosen = labels[idxs],preds[idxs]\n",
    "        mask = labels_chosen.sum(axis=0) > 0\n",
    "        ap = average_precision_score(labels_chosen[:,mask],preds_chosen[:,mask])\n",
    "        aps.append(ap)\n",
    "    return np.array(aps)\n",
    "\n",
    "def cale_f1_score(labels, preds,thresh=0.95):\n",
    "    f1s = []\n",
    "    num_experiments = 10\n",
    "    num_samples = int(0.9 * len(labels))\n",
    "    for _ in range(num_experiments):\n",
    "        idxs = np.random.randint(0,len(labels),num_samples)\n",
    "        labels_chosen, preds_chosen = labels[idxs],preds[idxs]\n",
    "        mask = labels_chosen.sum(axis=0) > 0\n",
    "        f1 = f1_score(labels_chosen[:,mask],preds_chosen[:,mask]>thresh,average='samples')\n",
    "        f1s.append(f1)\n",
    "    return np.array(f1s)\n",
    "\n",
    "metric_funcs = {'ap':calc_ap_score, 'top1_acc': calc_top1_acc}#, 'f1':cale_f1_score}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ap\n",
      "Beauty\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41dfc913028e480fa287b1a5c6ce5f80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            0     1     2     3     4     5     6     7     8     9\n",
      "no_cf    0.07  0.25  0.28  0.31  0.32  0.34  0.34  0.36  0.36  0.36\n",
      "with_cf  0.05  0.25  0.28  0.30  0.33  0.34  0.34  0.35  0.36  0.37\n",
      "Toys\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50eebee8182e4d3083db355e46d7a12c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            0     1    2     3     4     5     6     7     8     9\n",
      "no_cf    0.08  0.17  0.3  0.32  0.35  0.37  0.38  0.40  0.41  0.41\n",
      "with_cf  0.10  0.19  0.3  0.33  0.37  0.39  0.40  0.41  0.42  0.42\n",
      "Clothing\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6b07ed7fda041b4be67f9098966f0e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if osp.exists(metric_res_dicts_path):\n",
    "    metric_res_dicts= np.load(metric_res_dicts_path,allow_pickle=True).item()\n",
    "else:\n",
    "    metric_res_dicts = {}\n",
    "\n",
    "for metric_name, metric_func in metric_funcs.items():\n",
    "    print(metric_name)\n",
    "\n",
    "    # Initilize output: if metric exsits, use previous results\n",
    "    single_metric_res_dict = {}\n",
    "    if metric_name in metric_res_dicts:\n",
    "        single_metric_res_dict = metric_res_dicts[metric_name]\n",
    "\n",
    "    # metric -> dataset -> performance dataframe\n",
    "    single_metric_res_dict = compare_results(preds_dict, metric_func,single_metric_res_dict)\n",
    "\n",
    "    # Add to dict\n",
    "    metric_res_dicts[metric_name] = single_metric_res_dict\n",
    "    np.save(metric_res_dicts_path,metric_res_dicts)\n",
    "    print()\n",
    "\n",
    "np.save(metric_res_dicts_path,metric_res_dicts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric_name, single_metric_res_dict in metric_res_dicts.items():\n",
    "    print(metric_name)\n",
    "    fig, axs = plt.subplots(1, 4, figsize=(6, 1.5))\n",
    "    for row, (dataset_name, df) in enumerate(single_metric_res_dict.items()):\n",
    "        ax = axs[row]\n",
    "\n",
    "        ax.plot(df.index, df[\"no_cf\"], label=\"Baseline\")\n",
    "        ax.plot(df.index, df[\"with_cf\"], label=\"With CF\")\n",
    "\n",
    "        for i, key in enumerate([\"no_cf\", \"with_cf\"]):\n",
    "            ax.fill_between(\n",
    "                df.index,\n",
    "                df[key] - df[key + \"_ci\"],\n",
    "                df[key] + df[key + \"_ci\"],\n",
    "                color=f\"C{i}\",\n",
    "                alpha=0.1,\n",
    "            )\n",
    "\n",
    "        ax.set_xlabel(\"Label ratio \\n\\n ({}) {}\".format(chr(row + 97), dataset_name))\n",
    "    axs[0].legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    fig, axs = plt.subplots(1, 4, figsize=(6, 1.5))\n",
    "    for row, (dataset_name, df) in enumerate(single_metric_res_dict.items()):\n",
    "        ax = axs[row]\n",
    "\n",
    "        ax.bar(df.index, df[\"improvement\"], width=0.075)\n",
    "        ax.set_xlabel(\"Label ratio \\n\\n ({}) {}\".format(chr(row + 97), dataset_name))\n",
    "    axs[0].set_ylabel(\"Relative \\n improvement (\\%)\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_recall_at_k(labels, preds,k: int = 5):\n",
    "    recall_sum, item_num = 0, 0\n",
    "\n",
    "    recalls = []\n",
    "    for pred, label in zip(torch.tensor(preds), torch.tensor(labels)):\n",
    "        _, pred_idx = torch.topk(pred, k=k)  # The predicted labels\n",
    "        label_idx = torch.where(label == 1)[0]  # The ground truth labels\n",
    "\n",
    "        # In case there are no labels\n",
    "        if len(label_idx) == 0:\n",
    "            continue\n",
    "\n",
    "        # Recal per item\n",
    "        recall_i = sum(el in pred_idx for el in label_idx) / len(label_idx)\n",
    "\n",
    "        recalls.append(recall_i)\n",
    "\n",
    "    return recalls\n",
    "\n",
    "df = compare_results(labels, no_cf_preds, with_cf_preds, compute_recall_at_k)\n",
    "\n",
    "ax = plot_performance(df)\n",
    "ax.set_ylabel('Recall@5')\n",
    "plt.show()\n",
    "\n",
    "print(df.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_recall_at_k(labels, preds,k: int = 10):\n",
    "    recall_sum, item_num = 0, 0\n",
    "\n",
    "    recalls = []\n",
    "    for pred, label in zip(torch.tensor(preds), torch.tensor(labels)):\n",
    "        _, pred_idx = torch.topk(pred, k=k)  # The predicted labels\n",
    "        label_idx = torch.where(label == 1)[0]  # The ground truth labels\n",
    "\n",
    "        # In case there are no labels\n",
    "        if len(label_idx) == 0:\n",
    "            continue\n",
    "\n",
    "        # Recal per item\n",
    "        recall_i = sum(el in pred_idx for el in label_idx) / len(label_idx)\n",
    "\n",
    "        recalls.append(recall_i)\n",
    "\n",
    "    return recalls\n",
    "\n",
    "df = compare_results(labels, no_cf_preds, with_cf_preds, compute_recall_at_k)\n",
    "\n",
    "ax = plot_performance(df)\n",
    "ax.set_ylabel('Recall@10')\n",
    "plt.show()\n",
    "\n",
    "print(df.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_f1_score(labels, preds,thresh=0.95):\n",
    "    f1s = []\n",
    "    num_experiments = 75\n",
    "    num_samples = int(0.9 * len(labels))\n",
    "    for _ in range(num_experiments):\n",
    "        idxs = np.random.randint(0,len(labels),num_samples)\n",
    "        labels_chosen, preds_chosen = labels[idxs],preds[idxs]\n",
    "        mask = labels_chosen.sum(axis=0) > 0\n",
    "        f1 = f1_score(labels_chosen[:,mask],preds_chosen[:,mask]>thresh,average='samples')\n",
    "        f1s.append(f1)\n",
    "    return np.array(f1s)\n",
    "\n",
    "df = compare_results(labels, no_cf_preds, with_cf_preds, compute_f1_score)\n",
    "\n",
    "ax = plot_performance(df)\n",
    "ax.set_ylabel('F1 score')\n",
    "plt.show()\n",
    "\n",
    "print(df.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "93276fba55acec72ebf09ca3f12064a0f60315826507a886c17c3c32ba5acdb2"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('cactus': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
